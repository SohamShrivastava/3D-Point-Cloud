# Differential Transformer for 3D Point Cloud Upsampling

This project aims to improve 3D point cloud upsampling by integrating the **Differential Transformer** into the **PU-Transformer** architecture (ACCV 2022).

## Objective
To enhance point cloud detail and accuracy by replacing the standard self-attention block in PU-Transformer with a **Differential Attention** mechanism, which focuses on the relative differences between points.

## Planned Tasks
- Study PU-Transformer architecture and dataset setup.
- Understand and adapt Differential Transformer code.
- Replace attention modules and train the modified network.
- Evaluate using Chamfer Distance and visual comparison.

## References
- [PU-Transformer (Qiu et al., ACCV 2022)](https://openaccess.thecvf.com/content/ACCV2022/papers/Qiu_PU-Transformer_Point_Cloud_Upsampling_Transformer_ACCV_2022_paper.pdf)
- [Differential Transformer GitHub](https://github.com/paulilioaica/Differential-Transformer)

## Status
*Project ongoing â€“ Initial analysis and integration in progress.*

This project is being developed by [Soham Shrivastava](https://github.com/SohamShrivastava) and [Arin Mehta](https://github.com/ArinMehta) as part of an academic initiative at IIT Gandhinagar under the guidance of Prof. Shanmuganathan Raman.
