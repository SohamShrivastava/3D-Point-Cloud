# Differential Transformer for 3D Point Cloud Upsampling

This project aims to improve 3D point cloud upsampling by integrating the **Differential Transformer** into the **PU-Transformer** architecture (ACCV 2022).

## 🔍 Objective
To enhance point cloud detail and accuracy by replacing the standard self-attention block in PU-Transformer with a **Differential Attention** mechanism, which focuses on the relative differences between points.

## 🛠️ Planned Tasks
- Study PU-Transformer architecture and dataset setup.
- Understand and adapt Differential Transformer code.
- Replace attention modules and train the modified network.
- Evaluate using Chamfer Distance and visual comparison.

## 📚 References
- [PU-Transformer (Qiu et al., ACCV 2022)](https://openaccess.thecvf.com/content/ACCV2022/papers/Qiu_PU-Transformer_Point_Cloud_Upsampling_Transformer_ACCV_2022_paper.pdf)
- [Differential Transformer GitHub](https://github.com/paulilioaica/Differential-Transformer)

## 📌 Status
🟡 *Project ongoing – Initial analysis and integration in progress.*
